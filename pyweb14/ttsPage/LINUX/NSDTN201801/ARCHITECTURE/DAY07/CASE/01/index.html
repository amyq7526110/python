<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>CASE</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <script type="text/javascript" src="index.files/jquery.min.js">
    </script>
    <script type="text/javascript" src="index.files/jquery.snippet.js">
    </script>
    <script type="text/javascript" src="index.files/main.js">
    </script>
    <link type="text/css" href="index.files/index.css" rel="Stylesheet" />
    <link type="text/css" href="index.files/jquery.snippet.css" rel="Stylesheet" />
  </head>
  <body>
    <div class="source_style_case">
      <a name="page_top_case" id="top_anchor" />
      <a id="link_top" href="#page_top_case">Top</a>
      <h1>NSD ARCHITECTURE DAY07</h1>
      <ol class="index">
        <li>
          <a href="#case1">案例1：Zookeeper安装</a>
        </li>
        <li>
          <a href="#case2">步骤一：安装Zookeeper</a>
        </li>
        <li>
          <a href="#case3">案例2：Kafka集群实验</a>
        </li>
        <li>
          <a href="#case4">案例3：Hadoop高可用</a>
        </li>
        <li>
          <a href="#case5">案例4：高可用验证</a>
        </li>
      </ol>
      <a name="case1">
      </a>
      <h2>1 案例1：Zookeeper安装</h2>
      <h3>1.1 问题</h3>
      <p>本案例要求：</p>
      <ul class="list">
        <li>搭建Zookeeper集群并查看各服务器的角色</li>
        <li>停止Leader并查看各服务器的角色</li>
      </ul>
      <h3>1.2 步骤</h3>
      <p>实现此案例需要按照如下步骤进行。</p>
      <a name="case2">
      </a>
      <h2>2 步骤一：安装Zookeeper</h2>
      <p>1）编辑/etc/hosts ,所有集群主机可以相互 ping 通（在nn01上面配置，同步到node1，node2，node3）</p>
      <pre class="code">[root@nn01 hadoop]# vim /etc/hosts192.168.1.21  nn01192.168.1.22  node1192.168.1.23  node2192.168.1.24  node3192.168.1.25  node4[root@nn01 hadoop]# for i in {22..24}  \do	\scp /etc/hosts 192.168.1.$i:/etc/	\done		//同步配置hosts       100%  253   639.2KB/s   00:00    hosts       100%  253   497.7KB/s   00:00    hosts       100%  253   662.2KB/s   00:00	</pre>
      <p>2）安装 java-1.8.0-openjdk-devel,由于之前的hadoop上面已经安装过，这里不再安装，若是新机器要安装</p>
      <p>3）zookeeper 解压拷贝到 /usr/local/zookeeper</p>
      <pre class="code">[root@nn01 ~]# tar -xf zookeeper-3.4.10.tar.gz [root@nn01 ~]# mv zookeeper-3.4.10 /usr/local/zookeeper</pre>
      <p>4）配置文件改名，并在最后添加配置</p>
      <pre class="code">[root@nn01 ~]# cd /usr/local/zookeeper/conf/[root@nn01 conf]# lsconfiguration.xsl  log4j.properties  zoo_sample.cfg[root@nn01 conf]# mv zoo_sample.cfg  zoo.cfg[root@nn01 conf]# chown root.root zoo.cfg[root@nn01 conf]# vim zoo.cfgserver.1=node1:2888:3888server.2=node2:2888:3888server.3=node3:2888:3888server.4=nn01:2888:3888:observer</pre>
      <p>5）拷贝 /usr/local/zookeeper 到其他集群主机</p>
      <pre class="code">[root@nn01 conf]# for i in {22..24}; do rsync -aSH --delete /usr/local/zookeeper/ 192.168.1.$i:/usr/local/zookeeper  -e 'ssh' &amp; done[4] 4956[5] 4957[6] 4958</pre>
      <p>6）创建 mkdir /tmp/zookeeper，每一台都要</p>
      <pre class="code">[root@nn01 conf]# mkdir /tmp/zookeeper[root@nn01 conf]# ssh node1 mkdir /tmp/zookeeper[root@nn01 conf]# ssh node2 mkdir /tmp/zookeeper[root@nn01 conf]# ssh node3 mkdir /tmp/zookeeper</pre>
      <p>7）创建 myid 文件，id 必须与配置文件里主机名对应的 server.(id) 一致</p>
      <pre class="code">[root@nn01 conf]# echo 4 &gt;/tmp/zookeeper/myid[root@nn01 conf]# ssh node1 'echo 1 &gt;/tmp/zookeeper/myid'[root@nn01 conf]# ssh node2 'echo 2 &gt;/tmp/zookeeper/myid'[root@nn01 conf]# ssh node3 'echo 3 &gt;/tmp/zookeeper/myid'</pre>
      <p>8）启动服务，单启动一台无法查看状态，需要启动全部集群以后才能查看状态，每一台上面都要手工启动（以nn01为例子）</p>
      <pre class="code">[root@nn01 conf]# /usr/local/zookeeper/bin/zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTED</pre>
      <p>注意：刚启动zookeeper查看状态的时候报错，启动的数量要保证半数以上，这时再去看就成功了</p>
      <p>9）查看状态</p>
      <pre class="code">[root@nn01 conf]# /usr/local/zookeeper/bin/zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo.cfgMode: observe[root@nn01 conf]# /usr/local/zookeeper/bin/zkServer.sh stop  //关闭之后查看状态其他服务器的角色ZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo.cfgStopping zookeeper ... STOPPED[root@nn01 conf]# yum -y install telnet[root@nn01 conf]# telnet node3 2181 Trying 192.168.1.24...Connected to node3.Escape character is '^]'.ruok		//发送imokConnection closed by foreign host.		//imok回应的结果</pre>
      <p>10）利用 api 查看状态（nn01上面操作）</p>
      <pre class="code">[root@nn01 conf]# /usr/local/zookeeper/bin/zkServer.sh start[root@nn01 conf]# vim api.sh#!/bin/bashfunction getstatus(){    exec 9&lt;&gt;/dev/tcp/$1/2181 2&gt;/dev/null    echo stat &gt;&amp;9    MODE=$(cat &lt;&amp;9 |grep -Po "(?&lt;=Mode:).*")    exec 9&lt;&amp;-    echo ${MODE:-NULL}}for i in node{1..3} nn01;do    echo -ne "${i}\t"    getstatus ${i}done[root@nn01 conf]# chmod 755 api.sh[root@nn01 conf]# ./api.sh node1	followernode2	leadernode3	follower nn01	observer</pre>
      <a name="case3">
      </a>
      <h2>3 案例2：Kafka集群实验</h2>
      <h3>3.1 问题</h3>
      <p>本案例要求：</p>
      <ul class="list">
        <li>利用Zookeeper搭建一个Kafka集群</li>
        <li>创建一个topic</li>
        <li>模拟生产者发布消息</li>
        <li>模拟消费者接收消息</li>
      </ul>
      <h3>3.2 步骤</h3>
      <p>实现此案例需要按照如下步骤进行。</p>
      <p class="number">步骤一：搭建Kafka集群</p>
      <p>1）解压 kafka 压缩包</p>
      <p>Kafka在node1，node2，node3上面操作即可</p>
      <pre class="code">[root@node1 ~]# tar -xf kafka_2.10-0.10.2.1.tgz</pre>
      <p>2）把 kafka 拷贝到 /usr/local/kafka 下面</p>
      <pre class="code">[root@node1 ~]# mv kafka_2.10-0.10.2.1 /usr/local/kafka</pre>
      <p>3）修改配置文件 /usr/local/kafka/config/server.properties</p>
      <pre class="code">[root@node1 ~]# cd /usr/local/kafka/config[root@node1 config]# vim server.propertiesbroker.id=22zookeeper.connect=node1:2181,node2:2181,node3:2181</pre>
      <p>4）拷贝 kafka 到其他主机，并修改 broker.id ,不能重复</p>
      <pre class="code">[root@node1 config]# for i in 23 24; do rsync -aSH --delete /usr/local/kafka 192.168.1.$i:/usr/local/; done[1] 27072[2] 27073[root@node2 ~]# vim /usr/local/kafka/config/server.properties		//node2主机修改broker.id=23[root@node3 ~]# vim /usr/local/kafka/config/server.properties		//node3主机修改broker.id=24</pre>
      <p>5）启动 kafka 集群（node1，node2，node3启动）</p>
      <pre class="code">[root@node1 local]# /usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties [root@node1 local]# jps		//出现kafka26483 DataNode27859 Jps27833 Kafka26895 QuorumPeerMain</pre>
      <p>6）验证配置，创建一个 topic</p>
      <pre class="code">[root@node1 local]# /usr/local/kafka/bin/kafka-topics.sh --create --partitions 1 --replication-factor 1 --zookeeper node3:2181 --topic aa	Created topic "aa".</pre>
      <p>7) 模拟生产者，发布消息</p>
      <pre class="code">[root@node2 ~]# /usr/local/kafka/bin/kafka-console-producer.sh \--broker-list node2:9092 --topic aa		//写一个数据cccddd</pre>
      <p>9）模拟消费者，接收消息</p>
      <pre class="code">[root@node3 ~]# /usr/local/kafka/bin/kafka-console-consumer.sh \ --bootstrap-server node1:9092 --topic aa		//这边会直接同步cccddd</pre>
      <p>注意：kafka比较吃内存，做完这个kafka的实验可以把它停了</p>
      <a name="case4">
      </a>
      <h2>4 案例3：Hadoop高可用</h2>
      <h3>4.1 问题</h3>
      <p>本案例要求：</p>
      <ul class="list">
        <li>配置Hadoop的高可用</li>
        <li>修改配置文件</li>
      </ul>
      <h3>4.2 方案</h3>
      <p>配置Hadoop的高可用，解决NameNode单点故障问题，使用之前搭建好的hadoop集群，新添加一台nn02，ip为192.168.1.25，之前有一台node4主机，可以用这台主机，具体要求如图-1所示：</p>
      <div class="image">
        <img src="index.files/image001.png" />
        <p>图-1</p>
      </div>
      <h3>4.3 步骤</h3>
      <p>实现此案例需要按照如下步骤进行。</p>
      <p class="number">步骤一：hadoop的高可用</p>
      <p>1）停止所有服务（由于 kafka的实验做完之后就已经停止，这里不在重复）</p>
      <pre class="code">[root@nn01 ~]# cd /usr/local/hadoop/[root@nn01 hadoop]# ./sbin/stop-all.sh  //停止所有服务 </pre>
      <p>2）启动zookeeper（需要一台一台的启动）这里以nn01为例子</p>
      <pre class="code">[root@nn01 hadoop]# /usr/local/zookeeper/bin/zkServer.sh start[root@nn01 hadoop]# sh /usr/local/zookeeper/conf/api.sh //利用之前写好的脚本查看node1	followernode2	leadernode3	followernn01	observer</pre>
      <p>3）新加一台机器nn02，这里之前有一台node4，可以用这个作为nn02</p>
      <pre class="code">[root@node4 ~]# echo nn02 &gt; /etc/hostname [root@node4 ~]# hostname nn02</pre>
      <p>4）修改vim /etc/hosts</p>
      <pre class="code">[root@nn01 hadoop]# vim /etc/hosts192.168.1.21  nn01192.168.1.25  nn02192.168.1.22  node1192.168.1.23  node2192.168.1.24  node3</pre>
      <p>5）同步到nn02，node1，node2，node3</p>
      <pre class="code">[root@nn01 hadoop]# for i in {22..25}; do rsync -aSH --delete /etc/hosts 192.168.1.$i:/etc/hosts  -e 'ssh' &amp; done[1] 14355[2] 14356[3] 14357[4] 14358</pre>
      <p>6）配置SSH信任关系</p>
      <p>注意：nn01和nn02互相连接不需要密码，nn02连接自己和node1，node2，node3同样不需要密码</p>
      <pre class="code">[root@nn02 ~]# vim /etc/ssh/ssh_configHost *        GSSAPIAuthentication yes        StrictHostKeyChecking no[root@nn01 hadoop]# cd /root/.ssh/[root@nn01 .ssh]# scp id_rsa id_rsa.pub  nn02:/root/.ssh/	//把nn01的公钥私钥考给nn02 </pre>
      <p>7）所有的主机删除/var/hadoop/*</p>
      <pre class="code">[root@nn01 .ssh]# rm -rf /var/hadoop/*[root@nn01 .ssh]# ssh nn02 rm -rf /var/hadoop/*[root@nn01 .ssh]# ssh node1 rm -rf /var/hadoop/*[root@nn01 .ssh]# ssh node2 rm -rf /var/hadoop/*[root@nn01 .ssh]# ssh node3 rm -rf /var/hadoop/*</pre>
      <p>8）配置 core-site</p>
      <pre class="code">[root@nn01 .ssh]# vim /usr/local/hadoop/etc/hadoop/core-site.xml&lt;configuration&gt;&lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://nsdcluster&lt;/value&gt;	//nsdcluster是随便起的名。相当于一个组，访问的时候访问这个组    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;        &lt;value&gt;/var/hadoop&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;        &lt;value&gt;node1:2181,node2:2181,node3:2181&lt;/value&gt;	//zookeepe的地址    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;hadoop.proxyuser.nfs.groups&lt;/name&gt;        &lt;value&gt;*&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;hadoop.proxyuser.nfs.hosts&lt;/name&gt;        &lt;value&gt;*&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</pre>
      <p>9）配置 hdfs-site</p>
      <pre class="code">[root@nn01 ~]# vim /usr/local/hadoop/etc/hadoop/hdfs-site.xml&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;dfs.replication&lt;/name&gt;        &lt;value&gt;2&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.nameservices&lt;/name&gt;        &lt;value&gt;nsdcluster&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.namenodes.nsdcluster&lt;/name&gt;				//nn1,nn2名称固定，是内置的变量，nsdcluster里面有nn1，nn2        &lt;value&gt;nn1,nn2&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.rpc-address.nsdcluster.nn1&lt;/name&gt;		//声明nn1 8020为通讯端口，是nn01的rpc通讯端口        &lt;value&gt;nn01:8020&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.rpc-address.nsdcluster.nn2&lt;/name&gt;		//声明nn2是谁，nn02的rpc通讯端口        &lt;value&gt;nn02:8020&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.http-address.nsdcluster.nn1&lt;/name&gt;	//nn01的http通讯端口        &lt;value&gt;nn01:50070&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.http-address.nsdcluster.nn2&lt;/name&gt; 	//nn01和nn02的http通讯端口        &lt;value&gt;nn02:50070&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;		//指定namenode元数据存储在journalnode中的路径        &lt;value&gt;qjournal://node1:8485;node2:8485;node3:8485/nsdcluster&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;			//指定journalnode日志文件存储的路径        &lt;value&gt;/var/hadoop/journal&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.client.failover.proxy.provider.nsdcluster&lt;/name&gt;	//指定HDFS客户端连接active namenode的java类        &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;					//配置隔离机制为ssh        &lt;value&gt;sshfence&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;	//指定密钥的位置        &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;		//开启自动故障转移        &lt;value&gt;true&lt;/value&gt;				    &lt;/property&gt;&lt;/configuration&gt;</pre>
      <p>10）配置yarn-site</p>
      <pre class="code">[root@nn01 ~]# vim /usr/local/hadoop/etc/hadoop/yarn-site.xml&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;     &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;		//rm1,rm2代表nn01和nn02        &lt;value&gt;rm1,rm2&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;        &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;        &lt;value&gt;node1:2181,node2:2181,node3:2181&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;        &lt;value&gt;yarn-ha&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;        &lt;value&gt;nn01&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;        &lt;value&gt;nn02&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</pre>
      <p>11）同步到nn02，node1，node2，node3</p>
      <pre class="code">[root@nn01 ~]# for i in {22..25}; do rsync -aSH --delete /usr/local/hadoop/ 192.168.1.$i:/usr/local/hadoop  -e 'ssh' &amp; done[1] 25411[2] 25412[3] 25413[4] 25414</pre>
      <p>12）删除所有机器上面的/user/local/hadoop/logs，方便排错</p>
      <pre class="code">[root@nn01 ~]# for i in {21..25}; do ssh 192.168.1.$i rm -rf /usr/local/hadoop/logs ; done</pre>
      <p>13）同步配置</p>
      <pre class="code">[root@nn01 ~]# for i in {22..25}; do rsync -aSH --delete /usr/local/hadoop 192.168.1.$i:/usr/local/hadoop -e 'ssh' &amp; done[1] 28235[2] 28236[3] 28237[4] 28238</pre>
      <a name="case5">
      </a>
      <h2>5 案例4：高可用验证</h2>
      <h3>5.1 问题</h3>
      <p>本案例要求：</p>
      <ul class="list">
        <li>初始化集群</li>
        <li>验证集群</li>
      </ul>
      <h3>5.2 步骤</h3>
      <p>实现此案例需要按照如下步骤进行。</p>
      <p class="number">步骤一：验证hadoop的高可用</p>
      <p>1）初始化ZK集群</p>
      <pre class="code">[root@nn01 ~]# /usr/local/hadoop/bin/hdfs zkfc -formatZK ...18/09/11 15:43:35 INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/nsdcluster in ZK    //出现Successfully即为成功...</pre>
      <p>2）在node1，node2，node3上面启动journalnode服务（以node1为例子）</p>
      <pre class="code">[root@node1 ~]# /usr/local/hadoop/sbin/hadoop-daemon.sh start journalnode starting journalnode, logging to /usr/local/hadoop/logs/hadoop-root-journalnode-node1.out[root@node1 ~]# jps29262 JournalNode26895 QuorumPeerMain29311 Jps</pre>
      <p>3）格式化，先在node1，node2，node3上面启动journalnode才能格式化</p>
      <pre class="code">[root@nn01 ~]# /usr/local/hadoop//bin/hdfs  namenode  -format   //出现Successfully即为成功[root@nn01 hadoop]# ls /var/hadoop/dfs</pre>
      <p>4）nn02数据同步到本地 /var/hadoop/dfs</p>
      <pre class="code">[root@nn02 ~]# cd /var/hadoop/[root@nn02 hadoop]# ls[root@nn02 hadoop]# rsync -aSH  nn01:/var/hadoop/  /var/hadoop/[root@nn02 hadoop]# lsdfs</pre>
      <p>5）初始化 JNS</p>
      <pre class="code">[root@nn01 hadoop]# /usr/local/hadoop/bin/hdfs namenode -initializeSharedEdits18/09/11 16:26:15 INFO client.QuorumJournalManager: Successfully started new epoch 1		//出现Successfully，成功开启一个节点</pre>
      <p>6）停止 journalnode 服务（node1，node2，node3）</p>
      <pre class="code">[root@node1 hadoop]# /usr/local/hadoop/sbin/hadoop-daemon.sh stop journalnodestopping journalnode[root@node1 hadoop]# jps29346 Jps26895 QuorumPeerMain</pre>
      <p class="number">步骤二：启动集群</p>
      <p>1）nn01上面操作</p>
      <pre class="code">[root@nn01 hadoop]# /usr/local/hadoop/sbin/start-all.sh  //启动所有集群This script is Deprecated. Instead use start-dfs.sh and start-yarn.shStarting namenodes on [nn01 nn02]nn01: starting namenode, logging to /usr/local/hadoop/logs/hadoop-root-namenode-nn01.outnn02: starting namenode, logging to /usr/local/hadoop/logs/hadoop-root-namenode-nn02.outnode2: starting datanode, logging to /usr/local/hadoop/logs/hadoop-root-datanode-node2.outnode3: starting datanode, logging to /usr/local/hadoop/logs/hadoop-root-datanode-node3.outnode1: starting datanode, logging to /usr/local/hadoop/logs/hadoop-root-datanode-node1.outStarting journal nodes [node1 node2 node3]node1: starting journalnode, logging to /usr/local/hadoop/logs/hadoop-root-journalnode-node1.outnode3: starting journalnode, logging to /usr/local/hadoop/logs/hadoop-root-journalnode-node3.outnode2: starting journalnode, logging to /usr/local/hadoop/logs/hadoop-root-journalnode-node2.outStarting ZK Failover Controllers on NN hosts [nn01 nn02]nn01: starting zkfc, logging to /usr/local/hadoop/logs/hadoop-root-zkfc-nn01.outnn02: starting zkfc, logging to /usr/local/hadoop/logs/hadoop-root-zkfc-nn02.outstarting yarn daemonsstarting resourcemanager, logging to /usr/local/hadoop/logs/yarn-root-resourcemanager-nn01.outnode2: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-root-nodemanager-node2.outnode1: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-root-nodemanager-node1.outnode3: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-root-nodemanager-node3.out</pre>
      <p>2）nn02上面操作</p>
      <pre class="code">[root@nn02 hadoop]# /usr/local/hadoop/sbin/yarn-daemon.sh start resourcemanagerstarting resourcemanager, logging to /usr/local/hadoop/logs/yarn-root-resourcemanager-nn02.out</pre>
      <p>3）查看集群状态</p>
      <pre class="code">[root@nn01 hadoop]# /usr/local/hadoop/bin/hdfs haadmin -getServiceState nn1active[root@nn01 hadoop]# /usr/local/hadoop/bin/hdfs haadmin -getServiceState nn2standby[root@nn01 hadoop]# /usr/local/hadoop/bin/yarn rmadmin -getServiceState rm1active[root@nn01 hadoop]# /usr/local/hadoop/bin/yarn rmadmin -getServiceState rm2standby</pre>
      <p>4）查看节点是否加入</p>
      <pre class="code">[root@nn01 hadoop]# /usr/local/hadoop/bin/hdfs dfsadmin -report...Live datanodes (3):    //会有三个节点...[root@nn01 hadoop]# /usr/local/hadoop/bin/yarn  node  -listTotal Nodes:3         Node-Id	     Node-State	Node-Http-Address	Number-of-Running-Containers     node2:43307	        RUNNING	       node2:8042	                           0     node1:34606	        RUNNING	       node1:8042	                           0     node3:36749	        RUNNING	       node3:8042	                           0</pre>
      <p class="number">步骤三：访问集群</p>
      <p>1）查看并创建</p>
      <pre class="code">[root@nn01 hadoop]# /usr/local/hadoop/bin/hadoop  fs -ls  /[root@nn01 hadoop]# /usr/local/hadoop/bin/hadoop  fs -mkdir /aa //创建aa[root@nn01 hadoop]# /usr/local/hadoop/bin/hadoop  fs -ls  /		//再次查看Found 1 itemsdrwxr-xr-x   - root supergroup          0 2018-09-11 16:54 /aa[root@nn01 hadoop]# /usr/local/hadoop/bin/hadoop  fs -put *.txt /aa[root@nn01 hadoop]# /usr/local/hadoop/bin/hadoop  fs -ls hdfs://nsdcluster/aa  //也可以这样查看Found 3 items-rw-r--r--  2 root supergroup 86424 2018-09-11 17:00 hdfs://nsdcluster/aa/LICENSE.txt-rw-r--r--  2 root supergroup 14978 2018-09-11 17:00 hdfs://nsdcluster/aa/NOTICE.txt-rw-r--r--  2 root supergroup 1366 2018-09-11 17:00 hdfs://nsdcluster/aa/README.txt</pre>
      <p>2）验证高可用，关闭 active namenode</p>
      <pre class="code">[root@nn01 hadoop]# /usr/local/hadoop/bin/hdfs haadmin -getServiceState nn1active[root@nn01 hadoop]# /usr/local/hadoop/sbin/hadoop-daemon.sh stop namenodestopping namenode[root@nn01 hadoop]# /usr/local/hadoop/bin/hdfs haadmin -getServiceState nn1      //再次查看会报错[root@nn01 hadoop]# /usr/local/hadoop/bin/hdfs haadmin -getServiceState nn2  //nn02由之前的standby变为activeactive[root@nn01 hadoop]# /usr/local/hadoop/bin/yarn rmadmin -getServiceState rm1active[root@nn01 hadoop]# /usr/local/hadoop/sbin/yarn-daemon.sh stop resourcemanager  //停止resourcemanager [root@nn01 hadoop]# /usr/local/hadoop/bin/yarn rmadmin -getServiceState rm2active</pre>
      <p>3） 恢复节点</p>
      <pre class="code">[root@nn01 hadoop]# /usr/local/hadoop/sbin/hadoop-daemon.sh start namenode       //启动namenode[root@nn01 hadoop]# /usr/local/hadoop/sbin/yarn-daemon.sh start resourcemanager //启动resourcemanager[root@nn01 hadoop]# /usr/local/hadoop/bin/hdfs haadmin -getServiceState nn1      //查看[root@nn01 hadoop]# /usr/local/hadoop/bin/yarn rmadmin -getServiceState rm1      //查看</pre>
    </div>
  </body>
</html>